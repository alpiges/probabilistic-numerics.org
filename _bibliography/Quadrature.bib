@article{1987,
 jstor_articletype = {research-article},
 title = {Monte Carlo is Fundamentally Unsound},
 author = {O'Hagan, A.},
 journal = {Journal of the Royal Statistical Society. Series D (The Statistician)},
 jstor_issuetitle = {Special Issue: Practical Bayesian Statistics},
 volume = {36},
 number = {2/3},
 jstor_formatteddate = {1987},
 pages = {pp. 247-249},
 link = {http://www.jstor.org/stable/2348519},
 ISSN = {00390526},
 abstract = {We present some fundamental objections to the Monte Carlo method of numerical integration.},
 language = {English},
 year = {1987},
 publisher = {Wiley for the Royal Statistical Society},
 copyright = {Copyright © 1987 Royal Statistical Society}
}

@article{kennedy1996iterative,
  title={Iterative rescaling for Bayesian quadrature},
  author={Kennedy, MC and O’Hagan, A},
  journal={Bayesian Statistics},
  volume={5},
  pages={639--645},
  year={1996},
  publisher={Oxford University Press Oxford}
}

@article{kennedy1998bayesian,
  title={Bayesian quadrature with non-normal approximating functions},
  author={Kennedy, Marc},
  journal={Statistics and Computing},
  volume={8},
  number={4},
  pages={365--375},
  year={1998},
  link={http://dl.acm.org/citation.cfm?id=599295},
  publisher={Springer}
}


@article{o1991bayes,
  title =	 {{B}ayes--{H}ermite quadrature},
  author =	 {O'Hagan, A.},
  journal =	 {Journal of statistical planning and inference},
  volume =	 29,
  number =	 3,
  pages =	 {245--260},
  year =	 1991,
  abstract =	 {Bayesian quadrature treats the problem of numerical
                  integration as one of statistical inference. A prior Gaussian
                  process distribution is assumed for the integrand,
                  observations arise from evaluating the integrand at selected
                  points, and a posterior distribution is derived for the
                  integrand and the integral. Methods are developed for
                  quadrature in p. A particular application is integrating the
                  posterior density arising from some other Bayesian analysis.
                  Simulation results are presented, to show that the resulting
                  Bayes–Hermite quadrature rules may perform better than the
                  conventional Gauss–Hermite rules for this application. A key
                  result is derived for product designs, which makes Bayesian
                  quadrature practically useful for integrating in several
                  dimensions. Although the method does not at present provide a
                  solution to the more difficult problem of quadrature in high
                  dimensions, it does seem to offer real improvements over
                  existing methods in relatively low dimensions.}
}

@techreport{minka2000deriving,
  author =	 {T.P. Minka},
  institution =	 {Statistics Department, Carnegie Mellon University},
  title =	 {{Deriving quadrature rules from {G}aussian processes}},
  year =	 2000,
  link = {http://research.microsoft.com/en-us/um/people/minka/papers/quadrature.html},
  abstract =	 {Quadrature rules are often designed to achieve zero error on
                  a small set of functions, e.g. polynomials of specified
                  degree. A more robust method is to minimize average error
                  over a large class or distribution of functions. If functions
                  are distributed according to a Gaussian process, then
                  designing an average-case quadrature rule reduces to solving
                  a system of 2n equations, where n is the number of nodes in
                  the rule (O'Hagan, 1991). It is shown how this very general
                  technique can be used to design customized quadrature rules,
                  in the style of Yarvin & Rokhlin (1998), without the need for
                  singular value decomposition and in any number of
                  dimensions. It is also shown how classical Gaussian
                  quadrature rules, trigonometric lattice rules, and spline
                  rules can be extended to the average-case and to multiple
                  dimensions by deriving them from Gaussian processes. In
                  addition to being more robust, multidimensional quadrature
                  rules designed for the average-case are found to be much less
                  ambiguous than those designed for a given polynomial degree.}
}

@inproceedings{ghahramani2002bayesian,
  title={Bayesian {Monte Carlo}},
  author={Ghahramani, Zoubin and Rasmussen, Carl E},
  booktitle={Advances in neural information processing systems},
  pages={489--496},
  file={http://machinelearning.wustl.edu/mlpapers/paper_files/AA01.pdf},
  year={2002},
  abstract = { We investigate Bayesian alternatives to classical Monte Carlo
    methods for evaluating integrals. Bayesian Monte Carlo (BMC) allows the in-
    corporation of prior knowledge, such as smoothness of the integrand, into
    the estimation. In a simple problem we show that this outperforms any
    classical importance sampling method. We also attempt more chal- lenging
    multidimensional integrals involved in computing marginal like- lihoods of
    statistical models (a.k.a. partition functions and model evi- dences). We
    find that Bayesian Monte Carlo outperformed Annealed Importance Sampling,
    although for very high dimensional problems or problems with massive
    multimodality BMC may be less adequate. One advantage of the Bayesian
    approach to Monte Carlo is that samples can be drawn from any distribution.
    This allows for the possibility of active design of sample points so as to
    maximise information gain. }
}


@inproceedings{osborne2012bayesian,
  author =	 {M.A. Osborne and R. Garnett and S.J. Roberts and C. Hart and
                  S.  Aigrain and N. Gibson},
  booktitle =	 {{International Conference on Artificial Intelligence and
                  Statistics}},
  pages =	 {832--840},
  title =	 {{Bayesian quadrature for ratios}},
  year =	 2012,
  abstract =	 {We describe a novel approach to quadrature for ratios of
                  probabilistic integrals, such as are used to compute
                  posterior probabilities. This approach offers performance
                  superior to Monte Carlo methods by exploiting a Bayesian
                  quadrature framework. We improve upon previous Bayesian
                  quadrature techniques by explicitly modelling the
                  non-negativity of our integrands, and the correlations that
                  exist between them. It offers most where the integrand is
                  multi-modal and expensive to evaluate. We demonstrate the
                  efficacy of our method on data from the Kepler space
                  telescope.},
  file =	 {../assets/pdf/Osborne2012Bayesian.pdf}
}


@inproceedings{osborne2012active,
  author =	 {M.A. Osborne and D.K. Duvenaud and R. Garnett and
                  C.E. Rasmussen and S.J. Roberts and Z. Ghahramani},
  booktitle =	 {{Advances in Neural Information Processing Systems (NIPS)}},
  pages =	 {46--54},
  title =	 {{Active Learning of Model Evidence Using Bayesian
                  Quadrature.}},
  year =	 2012,
  abstract =	 { Numerical integration is a key component of many problems in
                  scientific computing, statistical modelling, and machine
                  learning. Bayesian Quadrature is a model-based method for
                  numerical integration which, relative to standard Monte Carlo
                  methods, offers increased sample efficiency and a more robust
                  estimate of the uncertainty in the estimated integral. We
                  propose a novel Bayesian Quadrature approach for numerical
                  integration when the integrand is non-negative, such as the
                  case of computing the marginal likelihood, predictive
                  distribution, or normalising constant of a probabilistic
                  model. Our approach approximately marginalises the quadrature
                  model’s hyperparameters in closed form, and introduces an ac-
                  tive learning scheme to optimally select function
                  evaluations, as opposed to using Monte Carlo samples. We
                  demonstrate our method on both a number of synthetic
                  benchmarks and a real scientific problem from astronomy.},
 file =	 {../assets/pdf/Osborne2012active.pdf
}
}

@inproceedings{sarkkagaussian,
  title =	 {Gaussian Process Quadratures in Nonlinear Sigma-Point
                  Filtering and Smoothing},
  author =	 {S{\"a}rkk{\"a}, Simo and Hartikainen, Jouni and Svensson,
                  Lennart and Sandblom, Fredrik},
  booktitle =	 {FUSION},
  year =	 2014,
  file =	 {http://becs.aalto.fi/~ssarkka/pub/gp_quad_fusion_2014.pdf},
  abstract =	 {This paper is concerned with the use of Gaussian process
                  regression based quadrature rules in the context of sigma-
                  point-based nonlinear Kalman filtering and smoothing. We show
                  how Gaussian process (i.e., Bayesian or Bayes–Hermite)
                  quadratures can be used for numerical solving of the
                  Gaussian integrals arising in the filters and smoothers. An
                  interesting additional result is that with suitable
                  selections of Hermite polynomial covariance functions the
                  Gaussian process quadratures can be reduced to unscented
                  transforms, spherical cubature rules, and to Gauss- Hermite
                  rules previously proposed for approximate nonlinear Kalman
                  filter and smoothing. Finally, the performance of the
                  Gaussian process quadratures in this context is evaluated
                  with numerical simulations. }
}

@InProceedings{gunter14-fast-bayesian-quadrature,
  author =	 {Tom Gunter and Michael A. Osborne and Roman Garnett and
                  Philipp Hennig and Stephen Roberts},
  title =	 {Sampling for Inference in Probabilistic Models with Fast
                  Bayesian Quadrature},
  booktitle =	 {Advances in Neural Information Processing Systems (NIPS)},
  year =	 2014,
  editor =	 {C. Cortes and N. Lawrence},
  abstract =	 {We propose a novel sampling framework for inference in
                  probabilistic models: an active learning approach that
                  converges more quickly (in wall-clock time) than Markov chain
                  Monte Carlo (MCMC) benchmarks. The central challenge in
                  probabilistic inference is numerical integration, to average
                  over ensembles of models or unknown (hyper-)parameters (for
                  example to compute marginal likelihood or a partition
                  function). MCMC has provided approaches to numerical
                  integration that deliver state-of-the-art inference, but can
                  suffer from sample inefficiency and poor convergence
                  diagnostics. Bayesian quadrature techniques offer a
                  model-based solution to such problems, but their uptake has
                  been hindered by prohibitive computation costs. We introduce
                  a warped model for probabilistic integrands (likelihoods)
                  that are known to be non-negative, permitting a cheap active
                  learning scheme to optimally select sample locations. Our
                  algorithm is demonstrated to offer faster convergence (in
                  seconds) relative to simple Monte Carlo and annealed
                  importance sampling on both synthetic and real-world
                  examples.},
  code =	 {https://github.com/OxfordML/wsabi}
}


@Article{oates14-contr-monte-carlo,
  author =	 {Chris J. Oates and Mark Girolami and Nicolas Chopin},
  title =	 {Control functionals for Monte Carlo integration},
  journal =	 {arXiv preprint 1410.2392},
  year =	 2014,
  abstract =	 {This paper introduces a novel class of estimators for Monte
                  Carlo integration, that leverage gradient information in
                  order to provide the improved estimator performance demanded
                  by contemporary statistical applications. The proposed
                  estimators, called "control functionals", achieve sub-root-n
                  convergence and often require orders of magnitude fewer
                  simulations, compared with existing approaches, in order to
                  achieve a fixed level of precision. We focus on a particular
                  sub-class of estimators that permit an elegant analytic form
                  and study their properties, both theoretically and
                  empirically. Results are presented on Bayes-Hermite
                  quadrature, hierarchical Gaussian process models and
                  non-linear ordinary differential equation models, where in
                  each case our estimators are shown to offer state of the art
                  performance.},
  link =          {http://www2.warwick.ac.uk/fac/sci/statistics/staff/academic-research/oates/control_functionals},
  file =	 {http://arxiv.org/pdf/1410.2392v1}
}

@ARTICLE{2015arXiv150405994S,
   author = {{S{\"a}rkk{\"a}}, S. and {Hartikainen}, J. and {Svensson}, L. and
  {Sandblom}, F.},
    title = "{On the relation between Gaussian process quadratures and sigma-point methods}",
  journal = {arXiv preprint stat.ME 1504.05994},
     year = 2015,
    month = apr,
    abstract = {This article is concerned with Gaussian process quadratures, which are numerical integration methods based on Gaussian process regression methods, and sigma-point methods, which are used in advanced non-linear Kalman filtering and smoothing algorithms. We show that many sigma-point methods can be interpreted as Gaussian quadrature based methods with suitably selected covariance functions. We show that this interpretation also extends to more general multivariate Gauss--Hermite integration methods and related spherical cubature rules. Additionally, we discuss different criteria for selecting the sigma-point locations: exactness for multivariate polynomials up to a given order, minimum average error, and quasi-random point sets. The performance of the different methods is tested in numerical experiments.},
    file = {http://arxiv.org/pdf/1504.05994v1.pdf}
}

@inproceedings{NIPS2014_5595,
title = {Just-In-Time Learning for Fast and Flexible Inference},
author = {Eslami, S. M. Ali and Tarlow, Daniel and Kohli, Pushmeet and Winn, John},
booktitle = {Advances in Neural Information Processing Systems (NIPS) 27},
editor = {Z. Ghahramani and M. Welling and C. Cortes and N.D. Lawrence and K.Q. Weinberger},
pages = {154--162},
year = {2014},
file = {http://papers.nips.cc/paper/5595-just-in-time-learning-for-fast-and-flexible-inference.pdf},
abstract = {Much of research in machine learning has centered around the search for inference algorithms that are both general-purpose and efficient. The problem is extremely challenging and general inference remains computationally expensive. We seek to address this problem by observing that in most specific applications of a model, we typically only need to perform a small subset of all possible inference computations. Motivated by this, we introduce just-in-time learning, a framework for fast and flexible inference that learns to speed up inference at run-time. Through a series of experiments, we show how this framework can allow us to combine the flexibility of sampling with the efficiency of deterministic message-passing.}
}



@InProceedings{Jitkrittum1503,
   author = {{Jitkrittum}, W. and {Gretton}, A. and {Heess}, N. and {Eslami}, S.~M.~A. and
  {Lakshminarayanan}, B. and {Sejdinovic}, D. and {Szab{\'o}}, Z.
  },
    title = "{Kernel-Based Just-In-Time Learning for Passing Expectation Propagation Messages}",
  booktitle = {Uncertainty in Artificial Intelligence (UAI) 31},
     year = 2015,
     file = {http://auai.org/uai2015/proceedings/papers/235.pdf},
     abstract  = {We propose an efficient nonparametric strategy for learning a message operator in expectation propagation (EP), which takes as input the set of incoming messages to a factor node, and produces an outgoing message as output. This learned operator replaces the multivariate integral required in classical EP, which may not have an analytic expression. We use kernel-based regression, which is trained on a set of probability distributions representing the incoming messages, and the associated outgoing messages. The kernel approach has two main advantages: first, it is fast, as it is implemented using a novel two-layer random feature representation of the input message distributions; second, it has principled uncertainty estimates, and can be cheaply updated online, meaning it can request and incorporate new training data when it encounters inputs on which it is uncertain. In experiments, our approach is able to solve learning problems where a single message operator is required for multiple, substantially different data sets (logistic regression for a variety of classification problems), where it is essential to accurately assess uncertainty and to efficiently and robustly update the message operator.}
}



@InProceedings{briol_frank-wolfe_2015,
  title = {Frank-{Wolfe} {Bayesian} {Quadrature}: {Probabilistic} {Integration} with {Theoretical} {Guarantees}},
  shorttitle = {Frank-{Wolfe} {Bayesian} {Quadrature}},
  url = {http://arxiv.org/abs/1506.02681},
  abstract = {There is renewed interest in formulating integration as an inference problem, motivated by obtaining a full distribution over numerical error that can be propagated through subsequent computation. Current methods, such as Bayesian Quadrature, demonstrate impressive empirical performance but lack theoretical analysis. An important challenge is to reconcile these probabilistic integrators with rigorous convergence guarantees. In this paper, we present the first probabilistic integrator that admits such theoretical treatment, called Frank-Wolfe Bayesian Quadrature (FWBQ). Under FWBQ, convergence to the true value of the integral is shown to be exponential and posterior contraction rates are proven to be superexponential. In simulations, FWBQ is competitive with state-of-the-art methods and out-performs alternatives based on Frank-Wolfe optimisation. Our approach is applied to successfully quantify numerical error in the solution to a challenging model choice problem in cellular biology.},
  booktitle =  {Advances in Neural Information Processing Systems (NIPS)},
  author = {Briol, François-Xavier and Oates, Chris J. and Girolami, Mark and Osborne, Michael A.},
  year = {2015},
  keywords = {Statistics - Machine Learning},
  file = {http://arxiv.org/pdf/1506.02681v1.pdf}
}

@article{bach2015equivalence,
  title={On the Equivalence between Quadrature Rules and Random Features},
  author={Bach, Francis},
  abstract={
    We show that kernel-based quadrature rules for computing in tegrals can be seen as a special case of random feature expansions for positive definite kernels, for a particular decomposition that always exists for such kernels. We provide a theoretical analysis of the number of required samples for a given approximation error, leading to both upper and lower bounds that are based solely on the eigenvalues of the associated integral operator and match up to logarithmic terms. In particular, we show that the upper bound may be obtained from independent an d identically distributed samples from a specific non-uniform distribution, while the lower bo und if valid for any set of points. Applying our results to kernel-based quadrature, while our results are fairly general, we recover known upper and lower bounds for the special cases of Sobolev spaces. Moreover, our results extend to the more general problem of full function approxim ations (beyond simply computing an integral), with results in L2- and L∞-norm that match known results for special cases.  Applying our results to random features, we show an improvement of the number of random features needed to preserve the generalization guarantees for learning with Lipshitz-continuous losses.
  },
  journal={arXiv preprint arXiv:1502.06800},
  file={http://arxiv.org/pdf/1502.06800v2.pdf},
  year={2015}
}


@article{briol_probabilistic_2015,
  title = {Probabilistic {Integration}},
  url = {http://arxiv.org/abs/1512.00933},
  abstract = {Probabilistic numerical methods aim to model numerical error as a source of epistemic uncertainty that is subject to probabilistic analysis and reasoning, enabling the principled propagation of numerical uncertainty through a computational pipeline. In this paper we focus on numerical methods for integration. We present probabilistic (Bayesian) versions of both Markov chain and Quasi Monte Carlo methods for integration and provide rigorous theoretical guarantees for convergence rates, in both posterior mean and posterior contraction. The performance of probabilistic integrators is guaranteed to be no worse than non-probabilistic integrators and is, in many cases, asymptotically superior. These probabilistic integrators therefore enjoy the "best of both worlds", leveraging the sampling efficiency of advanced Monte Carlo methods whilst being equipped with valid probabilistic models for uncertainty quantification. Several applications and illustrations are provided, including examples from computer vision and system modelling using non-linear differential equations. A survey of open challenges in probabilistic integration is provided.},
  urldate = {2015-12-04},
  journal = {arXiv:1512.00933 [cs, math, stat]},
  author = {Briol, François-Xavier and Oates, Chris J. and Girolami, Mark and Osborne, Michael A. and Sejdinovic, Dino},
  year = {2015},
  note = {arXiv: 1512.00933},
  file = {http://arxiv.org/pdf/1512.00933v1.pdf}
}
