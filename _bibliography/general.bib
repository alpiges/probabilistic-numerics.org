
@article{kac1940gaussian,
  title =	 {The Gaussian law of errors in the theory of additive number
                  theoretic functions},
  author =	 {Erd{\"o}s, P. and Kac, M},
  journal =	 {American Journal of Mathematics},
  pages =	 {738--742},
  year =	 1940,
  link =	 {http://www.jstor.org/stable/2371483}
}

@article{kac1949distributions,
  title =	 {On distributions of certain Wiener functionals},
  author =	 {Kac, M.},
  journal =	 {Transactions of the AMS},
  volume =	 65,
  number =	 1,
  pages =	 {1--13},
  year =	 1949,
  link =	 {http://www.jstor.org/stable/1990512}
}

@article{kimeldorf1970correspondence,
 title={A correspondence between Bayesian estimation on stochastic processes and smoothing by splines},
 author={Kimeldorf, George S and Wahba, Grace},
 journal={The Annals of Mathematical Statistics},
 volume={41},
 number={2},
 pages={495--502},
 year={1970},
 link={http://www.jstor.org/stable/pdf/2239347.pdf}
}

@article{larkin1972gaussian,
  title={Gaussian measure in {H}ilbert space and applications in numerical analysis},
  author={Larkin, F. M.},
  journal={Rocky Mountain Journal of Mathematics},
  volume={2},
  number={3},
  year={1972},
  pages={379--422},
  abstract = {The numerical analyst is often called upon to estimate a function from a very limited knowledge of its properties (e.g. a finite number of ordinate values). This problem may be made well posed in a variety of ways, but an attractive approach is to regard the required function as a member of a linear space on which a probability measure is constructed, and then use established techniques of probability theory and statistics in order to infer properties of the function from the given information. This formulation agrees with established theory, for the problem of optimal linear approximation (using a Gaussian probability distribution), and also permits the estimation of nonlinear functionals, as well as extension to the case of "noisy" data.}
}

@inproceedings{Kadane1985a,
author = {J. B. Kadane and G. W. Wasilkowski},
abstract = {Relations between average case epsilon-complexity and Bayesian statistics are discussed. An algorithm corresponds to a decision function, and the choice of information to the choice of an experiment. Adaptive information in epsilon-complexity theory corresponds to the concept of sequential experiment. Some results are reported, giving epsilon-complexity and minimax-Bayesian interpretations for factor analysis. Results from epsilon-complexity are used to establish the optimal sequential design is no better than optimal nonsequential design for that problem.},
booktitle = {Bayesian Statistics 2, Proceedings of the Second Valencia International Meeting},
number = {July},
pages = {361--374},
title = {{Average case epsilon-complexity in computer science: A Bayesian view}},
year = {1985},
link = {http://academiccommons.columbia.edu/catalog/ac%3A140709}
}

@article{Kadane1985b,
abstract = {I borrow themes from statistics - especially the Bayesian ideas underlying average case analysis and ideas of sequential design of experiments - to discuss when parallel computation is likely to be an attractive technique.},
author = {J. B. Kadane},
journal = {Journal of Complexity},
pages = {256--263},
title = {{Parallel and Sequential Computation: A Statistician's View}},
volume = {1},
year = {1985},
link = {http://www.sciencedirect.com/science/article/pii/0885064X85900147}
}


@article{diaconis1988bayesian,
  title =	 {Bayesian numerical analysis},
  author =	 {Diaconis, Persi},
  journal =	 {Statistical decision theory and related topics IV},
  volume =	 {1},
  pages =	 {163--175},
  year =	 {1988},
  publisher =	 {Springer-Verlag, New York},
  file =   {../assets/pdf/Diaconis_1988.pdf},
}

@article{o1992some,
  title={Some Bayesian numerical analysis},
  author={Oâ€™Hagan, Anthony},
  journal={Bayesian Statistics},
  volume={4},
  number={345--363},
  pages={4--2},
  file ={../assets/pdf/OHagan1991.pdf},
  year={1992}
}

@book{ritter2000average,
  title={Average-case analysis of numerical problems},
  author={Ritter, Klaus},
  number={1733},
  year={2000},
  publisher={Springer},
  series={Lecture Notes in Mathematics}
}

@article{HenOsbGirRSPA2015,
  author = {Hennig, Philipp and Osborne, Michael A. and Girolami, Mark},
  title = {Probabilistic numerics and uncertainty in computations},
  volume = {471},
  number = {2179},
  year = {2015},
  publisher = {The Royal Society},
  journal = {Proceedings of the Royal Society of London A: Mathematical, Physical and Engineering Sciences},
  abstract = {We deliver a call to arms for probabilistic numerical methods: algorithms for
numerical tasks, including linear algebra, integration, optimization and
solving differential equations, that return uncertainties in their
calculations. Such uncertainties, arising from the loss of precision induced by
numerical calculation with limited time or hardware, are important for much
contemporary science and industry. Within applications such as climate science
and astrophysics, the need to make decisions on the basis of computations with
large and complex data has led to a renewed focus on the management of
numerical uncertainty. We describe how several seminal classic numerical
methods can be interpreted naturally as probabilistic inference. We then show
that the probabilistic view suggests new algorithms that can flexibly be
adapted to suit application specifics, while delivering improved empirical
performance. We provide concrete illustrations of the benefits of probabilistic
numeric algorithms on real scientific problems from astrometry and astronomical
imaging, while highlighting open problems with these new algorithms. Finally,
we describe how probabilistic numerical methods provide a coherent framework
for identifying the uncertainty in calculations performed with a combination of
numerical algorithms (e.g. both numerical optimisers and differential equation
solvers), potentially allowing the diagnosis (and control) of error sources in
computations.},
  link = {http://rspa.royalsocietypublishing.org/content/471/2179/20150142},
  file = {http://rspa.royalsocietypublishing.org/content/royprsa/471/2179/20150142.full.pdf}
}

@incollection{Owhadi-Scovel-TowardsMachineWald,
  author = {Houman Owhadi and Clint Scovel},
  title = {Toward Machine {W}ald},
  booktitle = {Springer Handbook of Uncertainty Quantification},
  year = 2016,
  pages = {1--35},
  abstract = {The past century has seen a steady increase in the need of estimating and predicting complex systems and making (possibly critical) decisions with limited information. Although computers have made possible the numerical evaluation of sophisticated statistical models, these models are still designed by humans because there is currently no known recipe or algorithm for dividing the design of a statistical model into a sequence of arithmetic operations. Indeed enabling computers to think as humans, especially when faced with uncertainty, is challenging in several major ways: (1) Finding optimal statistical models remains to be formulated as a well-posed problem when information on the system of interest is incomplete and comes in the form of a complex combination of sample data, partial knowledge of constitutive relations and a limited description of the distribution of input random variables. (2) The space of admissible scenarios along with the space of relevant information, assumptions, and/or beliefs, tends to be infinite dimensional, whereas calculus on a computer is necessarily discrete and finite. With this purpose, this paper explores the foundations of a rigorous framework for the scientific computation of optimal statistical estimators/models and reviews their connections with decision theory, machine learning, Bayesian inference, stochastic optimization, robust optimization, optimal uncertainty quantification, and information-based complexity.},
  publisher = {Springer},
  link = {http://arxiv.org/abs/1508.02449},
  file = {http://arxiv.org/pdf/1508.02449v2.pdf}
}