@article{2014arXiv14022058H,
  author =	 {P. {Hennig}},
  journal =	 {SIAM J on Optimization},
  month =	 Jan,
  title =	 {{Probabilistic Interpretation of Linear Solvers}},
  year =	 2015,
  link =	 {http://epubs.siam.org/doi/abs/10.1137/140955501?journalCode=sjope8},
  volume =	 25,
  issue =	 1,
  abstract =	 {This paper proposes a probabilistic framework for algorithms
that iteratively solve unconstrained linear problems Bx = b with positive
definite B for x. The goal is to replace the point estimates returned by
existing methods with a Gaussian posterior belief over the elements of the
inverse of B, which can be used to estimate errors. Recent probabilistic
interpretations of the secant family of quasi-Newton optimization algorithms
are extended. Combined with properties of the conjugate gradient algorithm,
this leads to uncertainty-calibrated methods with very limited cost overhead
over conjugate gradients, a self-contained novel interpretation of the
quasi-Newton and conjugate gradient algorithms, and a foundation for new
nonlinear optimization methods.},
  file =	 {http://probabilistic-numerics.org/assets/pdf/HennigLinear2015.pdf}
}

@conference{BarHen16,
  title = {Probabilistic Approximate Least-Squares},
  author = {Bartels, S. and Hennig, P.},
  booktitle = {Proceedings of the 19th International Conference on Artificial Intelligence and Statistics (AISTATS 2016)},
  volume = {51},
  pages = {676--684},
  series = {JMLR Workshop and Conference Proceedings},
  editors = {Gretton, A. and Robert, C. C. },
  year = {2016},
  link = {http://jmlr.org/proceedings/papers/v51/bartels16.html},
  file = {http://jmlr.org/proceedings/papers/v51/bartels16.pdf},
  abstract = {Least-squares and kernel-ridge / Gaussian process regression are among the foundational algorithms of statistics and machine learning. Famously, the worst-case cost of exact nonparametric regression grows cubically with the data-set size; but a growing number of approximations have been developed that estimate good solutions at lower cost. These algorithms typically return point estimators, without measures of uncertainty. Leveraging recent results casting elementary linear algebra operations as probabilistic inference, we propose a new approximate method for nonparametric least-squares that affords a probabilistic uncertainty estimate over the error between the approximate and exact least-squares solution (this is not the same as the posterior variance of the associated Gaussian process regressor). This allows estimating the error of the least-squares solution on a subset of the data relative to the full-data solution. The uncertainty can be used to control the computational effort invested in the approximation. Our algorithm has linear cost in the data-set size, and a simple formal form, so that it can be implemented with a few lines of code in programming languages with linear algebra functionality.}
}
