@article{2014arXiv14022058H,
  author =	 {P. {Hennig}},
  journal =	 {SIAM J on Optimization},
  month =	 Jan,
  title =	 {{Probabilistic Interpretation of Linear Solvers}},
  year =	 2015,
  link =	 {http://epubs.siam.org/doi/abs/10.1137/140955501?journalCode=sjope8},
  volume =	 25,
  issue =	 1,
  abstract =	 {This paper proposes a probabilistic framework for algorithms
that iteratively solve unconstrained linear problems Bx = b with positive
definite B for x. The goal is to replace the point estimates returned by
existing methods with a Gaussian posterior belief over the elements of the
inverse of B, which can be used to estimate errors. Recent probabilistic
interpretations of the secant family of quasi-Newton optimization algorithms
are extended. Combined with properties of the conjugate gradient algorithm,
this leads to uncertainty-calibrated methods with very limited cost overhead
over conjugate gradients, a self-contained novel interpretation of the
quasi-Newton and conjugate gradient algorithms, and a foundation for new
nonlinear optimization methods.},
  file =	 {http://probabilistic-numerics.org/assets/pdf/HennigLinear2015.pdf}
}

@inproceedings{fitzsimons_bayesian_2017,
  title = {Bayesian {Inference} of {Log} {Determinants}},
  url = {https://arxiv.org/abs/1704.01445},
  abstract = {The log-determinant of a kernel matrix appears in a variety of machine learning problems, ranging from determinantal point processes and generalized Markov random fields, through to the training of Gaussian processes. Exact calculation of this term is often intractable when the size of the kernel matrix exceeds a few thousand. In the spirit of probabilistic numerics, we reinterpret the problem of computing the log-determinant as a Bayesian inference problem. In particular, we combine prior knowledge in the form of bounds from matrix theory and evidence derived from stochastic trace estimation to obtain probabilistic estimates for the log-determinant and its associated uncertainty within a given computational budget. Beyond its novelty and theoretic appeal, the performance of our proposal is competitive with state-of-the-art approaches to approximating the log-determinant, while also quantifying the uncertainty due to budget-constrained evidence.},
  urldate = {2017-06-21},
  booktitle = {Uncertainty in {Artificial} {Intelligence}},
  author = {Fitzsimons, Jack and Cutajar, Kurt and Osborne, Michael and Roberts, Stephen and Filippone, Maurizio},
  year = {2017},
  file = {http://probabilistic-numerics.org/assets/pdf/Fitzsimons et al. - 2017 - Bayesian Inference of Log Determinants.pdf}

@conference{BarHen16,
  title = {Probabilistic Approximate Least-Squares},
  author = {Bartels, S. and Hennig, P.},
  booktitle = {Proceedings of the 19th International Conference on Artificial Intelligence and Statistics (AISTATS 2016)},
  volume = {51},
  pages = {676--684},
  series = {JMLR Workshop and Conference Proceedings},
  editors = {Gretton, A. and Robert, C. C. },
  year = {2016},
  link = {http://jmlr.org/proceedings/papers/v51/bartels16.html},
  file = {http://jmlr.org/proceedings/papers/v51/bartels16.pdf},
  abstract = {Least-squares and kernel-ridge / Gaussian process regression are among the foundational algorithms of statistics and machine learning. Famously, the worst-case cost of exact nonparametric regression grows cubically with the data-set size; but a growing number of approximations have been developed that estimate good solutions at lower cost. These algorithms typically return point estimators, without measures of uncertainty. Leveraging recent results casting elementary linear algebra operations as probabilistic inference, we propose a new approximate method for nonparametric least-squares that affords a probabilistic uncertainty estimate over the error between the approximate and exact least-squares solution (this is not the same as the posterior variance of the associated Gaussian process regressor). This allows estimating the error of the least-squares solution on a subset of the data relative to the full-data solution. The uncertainty can be used to control the computational effort invested in the approximation. Our algorithm has linear cost in the data-set size, and a simple formal form, so that it can be implemented with a few lines of code in programming languages with linear algebra functionality.}
}
