@article{2014arXiv14071517B,
  author =	 {{Bui-Thanh}, T. and {Girolami}, M.},
  title =	 "{Solving Large-Scale PDE-constrained Bayesian Inverse
                  Problems with Riemann Manifold Hamiltonian Monte Carlo}",
  journal =	 {arXiv pre-print 1407.1517},
  keywords =	 {Mathematics - Statistics Theory},
  year =	 2014,
  month =	 July,
  abstract =	 {We consider the Riemann manifold Hamiltonian Monte Carlo
                  (RMHMC) method for solving statistical inverse problems
                  governed by partial differential equations (PDEs). The power
                  of the RMHMC method is that it exploits the geometric
                  structure induced by the PDE constraints of the underlying
                  inverse problem. Consequently, each RMHMC posterior sample is
                  almost independent from the others providing statistically
                  efficient Markov chain simulation. We reduce the cost of
                  forming the Fisher information matrix by using a low rank
                  approximation via a randomized singular value decomposition
                  technique. This is efficient since a small number of
                  Hessian-vector products are required. The Hessian-vector
                  product in turn requires only two extra PDE solves using the
                  adjoint technique. The results suggest RMHMC as a highly
                  efficient simulation scheme for sampling from PDE induced
                  posterior measures.},
  link =	 {http://arxiv.org/abs/1407.1517}
}

@article{conrad_probability_2015,
  title = {Probability {Measures} for {Numerical} {Solutions} of {Differential} {Equations}},
  file = {http://arxiv.org/pdf/1506.04592v1.pdf},
  link = {http://warwick.ac.uk/pints},
  abstract = {In this paper, we present a formal quantification of epistemic uncertainty induced by numerical solutions of ordinary and partial differential equation models. Numerical solutions of differential equations contain inherent uncertainties due to the finite dimensional approximation of an unknown and implicitly defined function. When statistically analysing models based on differential equations describing physical, or other naturally occurring, phenomena, it is therefore important to explicitly account for the uncertainty introduced by the numerical method. This enables objective determination of its importance relative to other uncertainties, such as those caused by data contaminated with noise or model error induced by missing physical or inadequate descriptors. To this end we show that a wide variety of existing solvers can be randomised, inducing a probability measure over the solutions of such differential equations. These measures exhibit contraction to a Dirac measure around the true unknown solution, where the rates of convergence are consistent with the underlying deterministic numerical method. Ordinary differential equations and elliptic partial differential equations are used to illustrate the approach to quantifying uncertainty in both the statistical analysis of the forward and inverse problems.},
  urldate = {2015-06-16},
  journal = {arXiv:1506.04592 [stat]},
  author = {Conrad, Patrick R. and Girolami, Mark and Särkkä, Simo and Stuart, Andrew and Zygalakis, Konstantinos},
  month = jun,
  year = {2015},
  note = {arXiv: 1506.04592},
  keywords = {Statistics - Methodology}
}


@article{raissi_machine_2017,
  title = {Machine {Learning} of {Linear} {Differential} {Equations} using {Gaussian} {Processes}},
  file = {https://arxiv.org/pdf/1701.02440.pdf},
  abstract = {This work leverages recent advances in probabilistic machine learning to discover conservation laws expressed by parametric linear equations. Such equations involve, but are not limited to, ordinary and partial differential, integro-differential, and fractional order operators. Here, Gaussian process priors are modified according to the particular form of such operators and are employed to infer parameters of the linear equations from scarce and possibly noisy observations. Such observations may come from experiments or "black-box" computer simulations.},
  journal = {arXiv:1701.02440 [cs, math, stat]},
  author = {Raissi, Maziar and Karniadakis, George Em},
  month = jan,
  year = {2017},
  note = {arXiv: 1701.02440},
  keywords = {Computer Science - Learning, Mathematics - Numerical Analysis, Statistics - Machine Learning}
}


@article{oates_bayesian_2017,
  title = {Bayesian {Probabilistic} {Numerical} {Methods} for {Industrial} {Process} {Monitoring}},
  url = {http://arxiv.org/abs/1707.06107},
  abstract = {The use of high-power industrial equipment, such as large-scale mixing equipment or a hydrocyclone for separation of particles in liquid suspension, demands careful monitoring to ensure correct operation. The task of monitoring the liquid suspension can be posed as a time-evolving inverse problem and solved with Bayesian statistical methods. In this paper, we extend Bayesian methods to incorporate statistical models for the error that is incurred in the numerical solution of the physical governing equations. This enables full uncertainty quantification within a principled computation-precision trade-off, in contrast to the over-confident inferences that are obtained when numerical error is ignored. The method is cast with a sequential Monte Carlo framework and an optimised implementation is provided in Python.},
  urldate = {2017-07-20},
  journal = {arXiv:1707.06107 [stat]},
  author = {Oates, Chris J. and Cockayne, Jon and Aykroyd, Robert G.},
  month = jul,
  year = {2017},
  note = {arXiv: 1707.06107},
  keywords = {Statistics - Applications},
  file = {https://arxiv.org/pdf/1707.06107.pdf}
}
